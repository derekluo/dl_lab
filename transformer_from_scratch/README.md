# Transformer From Scratch

本项目展示了如何从零开始实现基本的 Transformer 模型，用于文本分类任务。

## 项目结构

- `transformer.py` - 包含完整的 Transformer 模型实现
- `train.py` - 训练脚本和数据处理代码
- `README.md` - 项目说明文档

## 模型组件

### 1. MultiHeadAttention (多头注意力机制)
- 实现了缩放点积注意力
- 支持多头并行计算
- 包含线性投影和输出层

### 2. PositionalEncoding (位置编码)
- 使用正弦和余弦函数生成位置编码
- 为序列中的每个位置提供位置信息

### 3. FeedForward (前馈网络)
- 包含两个线性层和 ReLU 激活函数
- 在每个 Transformer 块中使用

### 4. TransformerBlock (Transformer 块)
- 结合多头注意力和前馈网络
- 包含残差连接和层归一化
- 支持掩码机制

### 5. Transformer (完整模型)
- 包含嵌入层、位置编码和多个 Transformer 块
- 用于文本分类任务的分类头
- 支持自定义的模型参数

## 使用方法

### 安装依赖
```bash
pip install torch torchvision numpy scikit-learn
```

### 运行训练
```bash
python train.py
```

### 模型参数
- `vocab_size`: 词汇表大小 (默认: 5000)
- `d_model`: 模型维度 (默认: 128)  
- `n_heads`: 注意力头数 (默认: 4)
- `n_layers`: Transformer 层数 (默认: 2)
- `num_classes`: 分类类别数 (默认: 5)
- `max_length`: 最大序列长度 (默认: 64)

## 训练数据

训练脚本使用合成的示例数据进行演示，包含5个不同的文本类别：
- 计算机/技术
- 体育
- 科学
- 音乐  
- 食物

在实际应用中，你可以替换为真实的文本数据集。

## 主要特性

1. **完整的 Transformer 实现**: 包含所有核心组件
2. **可配置的架构**: 支持自定义层数、头数、维度等
3. **文本分类任务**: 适用于多分类文本任务
4. **简单易懂**: 代码结构清晰，注释详细
5. **教学友好**: 适合学习 Transformer 原理

## 学习要点

通过这个项目，你将学习到：

1. **注意力机制**: 理解自注意力的计算过程
2. **位置编码**: 了解如何为序列添加位置信息
3. **残差连接**: 学习深度网络中的残差连接技巧
4. **层归一化**: 理解归一化在 Transformer 中的作用
5. **文本处理**: 学习简单的文本预处理和词汇表构建

## 扩展建议

- 尝试不同的数据集 (如 IMDB 电影评论、新闻分类等)
- 调整模型参数 (层数、头数、维度)
- 添加学习率调度器
- 实现 BERT 风格的预训练
- 添加序列到序列的功能

## 注意事项

- 这是一个教学用的简化版本，用于展示 Transformer 的核心原理
- 在实际生产环境中，建议使用如 `transformers` 库这样的成熟实现
- 模型参数相对较小，适合在 CPU 上运行演示
