# Backpropagation From Scratch

æœ¬é¡¹ç›®ä»é›¶å¼€å§‹å®ç°ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­ç®—æ³•ï¼Œè¯¦ç»†å±•ç¤ºäº†æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°çš„æ¯ä¸€ä¸ªæ­¥éª¤ã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ•™å­¦é¡¹ç›®ï¼Œç”¨äºæ·±å…¥ç†è§£ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒåŸç†ã€‚

## é¡¹ç›®ç»“æ„

- `model.py` - æ‰‹åŠ¨å®ç°çš„ç¥ç»ç½‘ç»œæ ¸å¿ƒç»„ä»¶
- `train.py` - è®­ç»ƒè„šæœ¬ï¼Œå±•ç¤ºä¸åŒä»»åŠ¡çš„è®­ç»ƒè¿‡ç¨‹
- `demo.py` - è¯¦ç»†æ¼”ç¤ºåå‘ä¼ æ’­çš„æ¯ä¸€æ­¥è®¡ç®—
- `visualize.py` - å¯è§†åŒ–è„šæœ¬ï¼Œç”Ÿæˆå„ç§å›¾è¡¨å’ŒåŠ¨ç”»
- `README.md` - é¡¹ç›®è¯´æ˜æ–‡æ¡£

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install numpy matplotlib scikit-learn networkx
```

### 2. è¿è¡Œè®­ç»ƒæ¼”ç¤º
```bash
python train.py
```
å±•ç¤ºäºŒåˆ†ç±»ã€å›å½’å’Œæ¿€æ´»å‡½æ•°å¯¹æ¯”ä¸‰ä¸ªå®Œæ•´ç¤ºä¾‹ã€‚

### 3. æŸ¥çœ‹è¯¦ç»†çš„åå‘ä¼ æ’­è¿‡ç¨‹
```bash
python demo.py
```
é€æ­¥å±•ç¤ºXORé—®é¢˜çš„æ±‚è§£è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¯ä¸€å±‚çš„æ¢¯åº¦è®¡ç®—ã€‚

### 4. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
```bash
python visualize.py
```
ç”Ÿæˆç½‘ç»œæ¶æ„å›¾ã€æŸå¤±æ›²é¢ã€æ¿€æ´»å‡½æ•°å›¾ç­‰å¤šç§å¯è§†åŒ–ã€‚

## æ ¸å¿ƒç‰¹æ€§

### ğŸ§  å®Œæ•´çš„ç¥ç»ç½‘ç»œå®ç°
- **æ¿€æ´»å‡½æ•°**: ReLU, Sigmoid, Tanh (åŒ…å«å‰å‘å’Œåå‘ä¼ æ’­)
- **æŸå¤±å‡½æ•°**: å‡æ–¹è¯¯å·® (MSE), äºŒå…ƒäº¤å‰ç†µ (BCE)
- **å…¨è¿æ¥å±‚**: æƒé‡åˆå§‹åŒ–ã€å‰å‘ä¼ æ’­ã€æ¢¯åº¦è®¡ç®—
- **ç½‘ç»œæ¶æ„**: çµæ´»çš„å±‚çº§ç»“æ„è®¾è®¡

### ğŸ“Š è¯¦ç»†çš„è®­ç»ƒåˆ†æ
- **å®æ—¶è®­ç»ƒç›‘æ§**: æŸå¤±ã€å‡†ç¡®ç‡ã€æ¢¯åº¦å¹…åº¦è·Ÿè¸ª
- **æƒé‡æ›´æ–°å¯è§†åŒ–**: æ˜¾ç¤ºæ¯å±‚æƒé‡çš„å˜åŒ–è¿‡ç¨‹
- **æ¢¯åº¦æµåˆ†æ**: æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜
- **å­¦ä¹ æ›²çº¿**: è®­ç»ƒè¿‡ç¨‹çš„å®Œæ•´å¯è§†åŒ–

### ğŸ” æ•™å­¦å‹å¥½çš„æ¼”ç¤º
- **é€æ­¥è®¡ç®—å±•ç¤º**: æ˜¾ç¤ºæ¯ä¸€å±‚çš„å‰å‘å’Œåå‘ä¼ æ’­è®¡ç®—
- **æ•°å­¦å…¬å¼å¯¹ç…§**: ä»£ç å®ç°ä¸ç†è®ºå…¬å¼çš„å¯¹åº”å…³ç³»
- **ç»å…¸é—®é¢˜æ±‚è§£**: XORé—®é¢˜çš„è¯¦ç»†è§£æè¿‡ç¨‹
- **å‚æ•°æ•æ„Ÿæ€§åˆ†æ**: ä¸åŒå­¦ä¹ ç‡å’Œæ¶æ„çš„å½±å“

## ä¸»è¦ç»„ä»¶è¯¦è§£

### model.py æ ¸å¿ƒå®ç°

#### æ¿€æ´»å‡½æ•°ç±»
```python
class ReLU(ActivationFunction):
    @staticmethod
    def forward(x):
        return np.maximum(0, x)
    
    @staticmethod
    def backward(x):
        return (x > 0).astype(float)
```

#### å…¨è¿æ¥å±‚
```python
class Dense(Layer):
    def forward(self, input_data):
        self.input = input_data
        return np.dot(input_data, self.weights) + self.bias
    
    def backward(self, output_gradient, learning_rate):
        # è®¡ç®—æƒé‡æ¢¯åº¦: âˆ‚L/âˆ‚W = input^T Ã— output_gradient
        weight_gradients = np.dot(self.input.T, output_gradient)
        # è®¡ç®—åç½®æ¢¯åº¦: âˆ‚L/âˆ‚b = sum(output_gradient)
        bias_gradients = np.sum(output_gradient, axis=0, keepdims=True)
        # è®¡ç®—è¾“å…¥æ¢¯åº¦: âˆ‚L/âˆ‚input = output_gradient Ã— W^T
        input_gradient = np.dot(output_gradient, self.weights.T)
        
        # æ›´æ–°å‚æ•°
        self.weights -= learning_rate * weight_gradients
        self.bias -= learning_rate * bias_gradients
        
        return input_gradient
```

#### å®Œæ•´ç½‘ç»œ
```python
class NeuralNetwork:
    def backward(self, y_true, y_pred, learning_rate):
        # ä»æŸå¤±å‡½æ•°å¼€å§‹è®¡ç®—åˆå§‹æ¢¯åº¦
        gradient = self.loss_function.backward(y_true, y_pred)
        
        # åå‘ä¼ æ’­æ¢¯åº¦
        for layer in reversed(self.layers):
            gradient = layer.backward(gradient, learning_rate)
```

## æ¼”ç¤ºæ¡ˆä¾‹

### 1. äºŒåˆ†ç±»é—®é¢˜ (åŒå¿ƒåœ†æ•°æ®é›†)
```python
# ç”Ÿæˆéçº¿æ€§å¯åˆ†çš„æ•°æ®
X, y = make_circles(n_samples=1000, noise=0.1, factor=0.5)

# åˆ›å»ºç½‘ç»œ
network = create_simple_classifier(input_size=2, hidden_size=10, output_size=1)

# è®­ç»ƒ
history = network.train(X, y, epochs=500, learning_rate=0.1)
```

**å­¦ä¹ è¦ç‚¹**:
- éçº¿æ€§æ¿€æ´»å‡½æ•°çš„å¿…è¦æ€§
- å†³ç­–è¾¹ç•Œçš„å½¢æˆè¿‡ç¨‹
- åˆ†ç±»å‡†ç¡®ç‡çš„æå‡æ›²çº¿

### 2. å›å½’é—®é¢˜ (æ­£å¼¦å‡½æ•°æ‹Ÿåˆ)
```python
# ç”Ÿæˆæ­£å¼¦å‡½æ•°æ•°æ®
X = np.linspace(-2*np.pi, 2*np.pi, 500).reshape(-1, 1)
y = np.sin(X) + noise

# åˆ›å»ºå›å½’ç½‘ç»œ
network = create_regression_network(input_size=1, hidden_sizes=[20, 20], output_size=1)
```

**å­¦ä¹ è¦ç‚¹**:
- å‡½æ•°é€¼è¿‘èƒ½åŠ›
- è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆç°è±¡
- ç½‘ç»œæ·±åº¦å¯¹æ‹Ÿåˆèƒ½åŠ›çš„å½±å“

### 3. XORé—®é¢˜ (ç»å…¸æ¼”ç¤º)
```python
# XORæ•°æ®é›†
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [1], [1], [0]])

# è¯¦ç»†å±•ç¤ºæ¯ä¸€æ­¥è®¡ç®—
network.detailed_forward(X, verbose=True)
network.detailed_backward(y, predictions, learning_rate, verbose=True)
```

**å­¦ä¹ è¦ç‚¹**:
- çº¿æ€§ä¸å¯åˆ†é—®é¢˜çš„è§£å†³
- éšè—å±‚çš„ä½œç”¨æœºåˆ¶
- æ¢¯åº¦å¦‚ä½•åœ¨ç½‘ç»œä¸­ä¼ æ’­

## å¯è§†åŒ–åŠŸèƒ½

### 1. ç½‘ç»œæ¶æ„å›¾
- ç¥ç»å…ƒå’Œè¿æ¥çš„å¯è§†åŒ–
- å±‚çº§ç»“æ„å±•ç¤º
- æ¿€æ´»å‡½æ•°æ ‡æ³¨

### 2. æ¿€æ´»å‡½æ•°åˆ†æ
- å‡½æ•°æ›²çº¿å’Œå¯¼æ•°æ›²çº¿
- æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ¼”ç¤º
- ä¸åŒæ¿€æ´»å‡½æ•°çš„æ¯”è¾ƒ

### 3. æŸå¤±æ›²é¢å¯è§†åŒ–
- 3DæŸå¤±æ›²é¢
- ç­‰é«˜çº¿å›¾
- æ¢¯åº¦ä¸‹é™è·¯å¾„

### 4. å­¦ä¹ åŠ¨æ€åˆ†æ
- ä¸åŒå­¦ä¹ ç‡çš„æ”¶æ•›è¡Œä¸º
- æƒé‡æ›´æ–°è½¨è¿¹
- è®­ç»ƒç¨³å®šæ€§åˆ†æ

### 5. æ¢¯åº¦æµå¯è§†åŒ–
- å„å±‚æ¢¯åº¦å¤§å°
- æƒé‡æ›´æ–°å¹…åº¦
- æ¢¯åº¦ä¼ æ’­æ–¹å‘

## æ•°å­¦åŸç†

### åå‘ä¼ æ’­æ ¸å¿ƒå…¬å¼

1. **é“¾å¼æ³•åˆ™åŸºç¡€**
   ```
   âˆ‚L/âˆ‚w_{ij}^{(l)} = âˆ‚L/âˆ‚a_j^{(l)} Ã— âˆ‚a_j^{(l)}/âˆ‚z_j^{(l)} Ã— âˆ‚z_j^{(l)}/âˆ‚w_{ij}^{(l)}
   ```

2. **æ¢¯åº¦é€’å½’å…³ç³»**
   ```
   Î´^{(l)} = (W^{(l+1)})^T Î´^{(l+1)} âŠ™ Ïƒ'(z^{(l)})
   ```

3. **å‚æ•°æ›´æ–°è§„åˆ™**
   ```
   W^{(l)} := W^{(l)} - Î± Ã— Î´^{(l+1)} Ã— (a^{(l)})^T
   b^{(l)} := b^{(l)} - Î± Ã— Î´^{(l+1)}
   ```

### æŸå¤±å‡½æ•°æ¢¯åº¦

**å‡æ–¹è¯¯å·® (MSE)**:
```
âˆ‚L/âˆ‚y_pred = 2(y_pred - y_true) / N
```

**äºŒå…ƒäº¤å‰ç†µ (BCE)**:
```
âˆ‚L/âˆ‚y_pred = -(y_true/y_pred - (1-y_true)/(1-y_pred)) / N
```

## å®éªŒç»“æœ

### æ€§èƒ½è¡¨ç°
- **XORé—®é¢˜**: 100% å‡†ç¡®ç‡ (3å±‚ç½‘ç»œï¼Œ50è½®è®­ç»ƒ)
- **åŒå¿ƒåœ†åˆ†ç±»**: ~95% æµ‹è¯•å‡†ç¡®ç‡ (éšè—å±‚10ä¸ªç¥ç»å…ƒ)
- **æ­£å¼¦å‡½æ•°æ‹Ÿåˆ**: MSE < 0.01 (2ä¸ªéšè—å±‚ï¼Œå„20ä¸ªç¥ç»å…ƒ)

### è®­ç»ƒæ•ˆç‡
- **CPUè®­ç»ƒ**: å°è§„æ¨¡é—®é¢˜ç§’çº§æ”¶æ•›
- **å†…å­˜ä½¿ç”¨**: åŸºäºNumPyï¼Œå†…å­˜æ•ˆç‡é«˜
- **æ•°å€¼ç¨³å®šæ€§**: åŒ…å«æ¢¯åº¦è£å‰ªå’Œæƒé‡åˆå§‹åŒ–

## æ•™å­¦ä»·å€¼

### é€‚åˆå­¦ä¹ è€…
- æœºå™¨å­¦ä¹ åˆå­¦è€…
- æ·±åº¦å­¦ä¹ è¯¾ç¨‹å­¦ç”Ÿ  
- æƒ³è¦ç†è§£åå‘ä¼ æ’­åŸç†çš„å¼€å‘è€…
- ç¥ç»ç½‘ç»œç ”ç©¶äººå‘˜

### å­¦ä¹ è·¯å¾„å»ºè®®
1. **åŸºç¡€æ¦‚å¿µ**: å…ˆè¿è¡Œ `train.py` äº†è§£æ•´ä½“æµç¨‹
2. **è¯¦ç»†åˆ†æ**: è¿è¡Œ `demo.py` æŸ¥çœ‹è®¡ç®—ç»†èŠ‚
3. **å¯è§†åŒ–ç†è§£**: è¿è¡Œ `visualize.py` ç”Ÿæˆå›¾è¡¨
4. **ä»£ç ç ”è¯»**: ä»”ç»†é˜…è¯» `model.py` çš„å®ç°
5. **å®éªŒæ‹“å±•**: å°è¯•ä¿®æ”¹ç½‘ç»œç»“æ„å’Œå‚æ•°

### æ‰©å±•å®éªŒå»ºè®®

#### 1. ç½‘ç»œæ¶æ„å®éªŒ
```python
# æ¯”è¾ƒä¸åŒéšè—å±‚å¤§å°
sizes = [5, 10, 20, 50]
for size in sizes:
    network = create_simple_classifier(2, size, 1)
    # è®­ç»ƒå¹¶æ¯”è¾ƒæ€§èƒ½
```

#### 2. å­¦ä¹ ç‡è°ƒåº¦
```python
# å®ç°å­¦ä¹ ç‡è¡°å‡
def train_with_schedule(network, X, y, initial_lr=0.1, decay=0.95):
    lr = initial_lr
    for epoch in range(epochs):
        network.train_step(X, y, lr)
        lr *= decay
```

#### 3. æ­£åˆ™åŒ–æŠ€æœ¯
```python
# æ·»åŠ L2æ­£åˆ™åŒ–
def l2_regularization(weights, lambda_reg):
    return lambda_reg * np.sum([np.sum(w**2) for w in weights])
```

## ä¸ç°ä»£æ¡†æ¶å¯¹æ¯”

### æœ¬é¡¹ç›®ä¼˜åŠ¿
- **é€æ˜æ€§**: æ¯ä¸€æ­¥è®¡ç®—éƒ½å¯è§å’Œå¯æ§
- **æ•™å­¦æ€§**: ä»£ç ç»“æ„æ¸…æ™°ï¼Œä¾¿äºç†è§£
- **è½»é‡çº§**: æ— å¤æ‚ä¾èµ–ï¼Œæ˜“äºä¿®æ”¹å’Œå®éªŒ

### PyTorchå¯¹æ¯”ç¤ºä¾‹
```python
# æœ¬é¡¹ç›®å®ç°
network = create_simple_classifier(2, 10, 1)
predictions = network.forward(X)
network.backward(y, predictions, 0.1)

# PyTorchå®ç°  
import torch.nn as nn
model = nn.Sequential(nn.Linear(2, 10), nn.ReLU(), nn.Linear(10, 1), nn.Sigmoid())
loss = nn.MSELoss()(model(X_torch), y_torch)
loss.backward()
```

## å¸¸è§é—®é¢˜è§£ç­”

### Q: ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ç°æˆçš„æ·±åº¦å­¦ä¹ æ¡†æ¶?
A: æœ¬é¡¹ç›®çš„ç›®çš„æ˜¯æ•™å­¦å’Œç†è§£ã€‚é€šè¿‡æ‰‹åŠ¨å®ç°ï¼Œå¯ä»¥æ·±å…¥ç†è§£æ¯ä¸ªç»„ä»¶çš„å·¥ä½œåŸç†ï¼Œè¿™å¯¹äºè°ƒè¯•å’Œä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹éå¸¸é‡è¦ã€‚

### Q: è¿™ä¸ªå®ç°çš„æ€§èƒ½å¦‚ä½•?
A: è¿™ä¸ªå®ç°ä¸“æ³¨äºå¯è¯»æ€§å’Œæ•™å­¦ä»·å€¼ï¼Œè€Œä¸æ˜¯æ€§èƒ½ã€‚å¯¹äºå¤§è§„æ¨¡é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨PyTorchæˆ–TensorFlowã€‚

### Q: å¦‚ä½•æ‰©å±•åˆ°æ›´å¤æ‚çš„æ¶æ„?
A: å¯ä»¥æ·»åŠ æ–°çš„å±‚ç±»å‹ï¼ˆå¦‚å·ç§¯å±‚ã€LSTMå±‚ï¼‰å’Œæ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆå¦‚Dropoutã€BatchNormï¼‰ã€‚ä»£ç ç»“æ„è®¾è®¡ä¸ºæ˜“äºæ‰©å±•ã€‚

### Q: æ•°å€¼ç¨³å®šæ€§å¦‚ä½•ä¿è¯?
A: å®ç°ä¸­åŒ…å«äº†æ¢¯åº¦è£å‰ªã€æƒé‡åˆå§‹åŒ–å’Œæ¿€æ´»å‡½æ•°è¾“å…¥èŒƒå›´é™åˆ¶ç­‰æŠ€æœ¯æ¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ã€‚

## å‚è€ƒèµ„æ–™

### ç†è®ºåŸºç¡€
- [Deep Learning (Ian Goodfellow)](http://www.deeplearningbook.org/)
- [Neural Networks and Deep Learning (Michael Nielsen)](http://neuralnetworksanddeeplearning.com/)

### æ•°å­¦èƒŒæ™¯
- [Matrix Calculus for Deep Learning](https://explained.ai/matrix-calculus/)
- [The Matrix Cookbook](https://www2.imm.dtu.dk/pubdb/pubs/3274-full.html)

### å®ç°å‚è€ƒ
- [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)
- [Neural Networks from Scratch](https://nnfs.io/)

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤æ”¹è¿›å»ºè®®å’Œé—®é¢˜æŠ¥å‘Šï¼å¯ä»¥æ”¹è¿›çš„æ–¹å‘ï¼š
- æ·»åŠ æ›´å¤šæ¿€æ´»å‡½æ•°ï¼ˆLeaky ReLU, Swishç­‰ï¼‰
- å®ç°æ‰¹é‡å½’ä¸€åŒ–å’ŒDropout
- æ·»åŠ æ›´å¤šä¼˜åŒ–ç®—æ³•ï¼ˆAdam, RMSpropç­‰ï¼‰
- åˆ›å»ºæ›´å¤šå¯è§†åŒ–åŠŸèƒ½
- æ”¹è¿›æ•°å€¼ç¨³å®šæ€§

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚æ¬¢è¿åœ¨æ•™å­¦å’Œç ”ç©¶ä¸­è‡ªç”±ä½¿ç”¨ã€‚